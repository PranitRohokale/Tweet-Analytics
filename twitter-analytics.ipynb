{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scrape Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import GetOldTweets3 as got\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# NTLK functions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize as tok\n",
    "from nltk.stem.snowball import SnowballStemmer # load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "# import lda # topic modeling -NMF & LDA\n",
    "import string\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "# Tf-Idf and Clustering packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAFYukQEAAAAApZQpuKapMCSBFMHu%2Ba1bySvK2EM%3DJNec4foagBf1eRvl240UJxO8SnkXL6mwWkQXr80HKxA1JBCkoy'\n",
    "client = tweepy.Client(bearer_token = bearer_token, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_terms = ['mortgage','current account','savings account','insurance','credit card','pension',\n",
    "                'personal loan','money transfer','tax advice','investment','wealth management']\n",
    "# search_terms = ['trump', 'gop', 'biden', 'democrats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mortgage\n",
      "current account\n",
      "savings account\n",
      "insurance\n",
      "credit card\n",
      "pension\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 507 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personal loan\n",
      "money transfer\n",
      "tax advice\n",
      "investment\n",
      "wealth management\n"
     ]
    }
   ],
   "source": [
    "max_results = 100\n",
    "expansions = ['author_id']\n",
    "user_fields = ['description','id','name','username']\n",
    "tweet_fields=['id', 'author_id', 'text', 'lang', 'public_metrics', 'created_at', 'entities']\n",
    "complete_data = {}\n",
    "for term in search_terms:\n",
    "    complete_data[term] = {}\n",
    "    fields = 0\n",
    "    print(term)\n",
    "    query = f'{term} lang:en -is:retweet -has:media'\n",
    "    for i,tweet_batch in  enumerate(tweepy.Paginator(client.search_recent_tweets, tweet_fields=tweet_fields,query=query, max_results=max_results, limit=100)):\n",
    "        complete_data[term][fields] = tweet_batch\n",
    "        fields+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotations': [{'start': 50,\n",
       "   'end': 66,\n",
       "   'probability': 0.5435,\n",
       "   'type': 'Other',\n",
       "   'normalized_text': 'kapila pashu ahar'}],\n",
       " 'mentions': [{'start': 0,\n",
       "   'end': 16,\n",
       "   'username': 'AbhasHalakhandi',\n",
       "   'id': '886946656423337985'}]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data['current account'][0].data[4].entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_df = pd.DataFrame(columns=['id','author_id','text','date', 'lang', 'search_term'])\n",
    "list_of_tweets = []\n",
    "for term in complete_data:\n",
    "    for i in complete_data[term]:\n",
    "\n",
    "        for tweet in complete_data[term][i].data:\n",
    "            id= tweet.id\n",
    "            author_id = tweet.author_id\n",
    "            text = tweet.text\n",
    "            date = tweet.created_at\n",
    "            lang = tweet.lang\n",
    "            # temp_df = pd.DataFrame({'id':[id], 'author_id':[author_id],'text':text, 'date':date, \"lang\":lang, 'search_term':term })\n",
    "            # tweet_df = tweet_df.append(temp_df)\n",
    "            temp = {'id':id, 'author_id':author_id,'text':text, 'date':date, \"lang\":lang, 'search_term':term }\n",
    "            list_of_tweets.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.DataFrame(list_of_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.read_csv(\"./data_with_search_term.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>search_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1602612821585694721</td>\n",
       "      <td>1580423829801009152</td>\n",
       "      <td>@AltMediaWatch @SmilyngAssassin @Tugboat882191...</td>\n",
       "      <td>2022-12-13 10:34:06+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1602612794360307714</td>\n",
       "      <td>1425092150975401996</td>\n",
       "      <td>@RalphNeale1 @CTVNews Trudeau had *nothing* to...</td>\n",
       "      <td>2022-12-13 10:33:59+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1602612758645805056</td>\n",
       "      <td>1556214967573118980</td>\n",
       "      <td>BREAKING Jag Singer threatens to give up pensi...</td>\n",
       "      <td>2022-12-13 10:33:51+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1602612676168798209</td>\n",
       "      <td>703144956</td>\n",
       "      <td>@wetthroatbabyy @nojumper Hes been paying a mo...</td>\n",
       "      <td>2022-12-13 10:33:31+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1602612625904394240</td>\n",
       "      <td>1425092150975401996</td>\n",
       "      <td>@khumphries1 @CTVNews @JustinTrudeau Trudeau h...</td>\n",
       "      <td>2022-12-13 10:33:19+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69838</th>\n",
       "      <td>69838</td>\n",
       "      <td>1600090654167687168</td>\n",
       "      <td>83786013</td>\n",
       "      <td>Job Japan Private Banker Wealth Management. 35...</td>\n",
       "      <td>2022-12-06 11:31:54+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>wealth management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69839</th>\n",
       "      <td>69839</td>\n",
       "      <td>1600089915387219968</td>\n",
       "      <td>3290091422</td>\n",
       "      <td>From Wealth Management Professional to Career ...</td>\n",
       "      <td>2022-12-06 11:28:58+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>wealth management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69840</th>\n",
       "      <td>69840</td>\n",
       "      <td>1600086634665431041</td>\n",
       "      <td>1587324685414322177</td>\n",
       "      <td>@WakeOfWeek @JayRocha82 @RobSchneider @Yankees...</td>\n",
       "      <td>2022-12-06 11:15:56+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>wealth management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69841</th>\n",
       "      <td>69841</td>\n",
       "      <td>1600081033373827073</td>\n",
       "      <td>993891818998771719</td>\n",
       "      <td>BMO Wealth Management Names NewLeadership http...</td>\n",
       "      <td>2022-12-06 10:53:41+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>wealth management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69842</th>\n",
       "      <td>69842</td>\n",
       "      <td>1600081028063821824</td>\n",
       "      <td>1117356436437315585</td>\n",
       "      <td>BMO Wealth Management Names NewLeadership http...</td>\n",
       "      <td>2022-12-06 10:53:39+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>wealth management</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69843 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                   id            author_id  \\\n",
       "0               0  1602612821585694721  1580423829801009152   \n",
       "1               1  1602612794360307714  1425092150975401996   \n",
       "2               2  1602612758645805056  1556214967573118980   \n",
       "3               3  1602612676168798209            703144956   \n",
       "4               4  1602612625904394240  1425092150975401996   \n",
       "...           ...                  ...                  ...   \n",
       "69838       69838  1600090654167687168             83786013   \n",
       "69839       69839  1600089915387219968           3290091422   \n",
       "69840       69840  1600086634665431041  1587324685414322177   \n",
       "69841       69841  1600081033373827073   993891818998771719   \n",
       "69842       69842  1600081028063821824  1117356436437315585   \n",
       "\n",
       "                                                    text  \\\n",
       "0      @AltMediaWatch @SmilyngAssassin @Tugboat882191...   \n",
       "1      @RalphNeale1 @CTVNews Trudeau had *nothing* to...   \n",
       "2      BREAKING Jag Singer threatens to give up pensi...   \n",
       "3      @wetthroatbabyy @nojumper Hes been paying a mo...   \n",
       "4      @khumphries1 @CTVNews @JustinTrudeau Trudeau h...   \n",
       "...                                                  ...   \n",
       "69838  Job Japan Private Banker Wealth Management. 35...   \n",
       "69839  From Wealth Management Professional to Career ...   \n",
       "69840  @WakeOfWeek @JayRocha82 @RobSchneider @Yankees...   \n",
       "69841  BMO Wealth Management Names NewLeadership http...   \n",
       "69842  BMO Wealth Management Names NewLeadership http...   \n",
       "\n",
       "                            date lang        search_term  \n",
       "0      2022-12-13 10:34:06+00:00   en           mortgage  \n",
       "1      2022-12-13 10:33:59+00:00   en           mortgage  \n",
       "2      2022-12-13 10:33:51+00:00   en           mortgage  \n",
       "3      2022-12-13 10:33:31+00:00   en           mortgage  \n",
       "4      2022-12-13 10:33:19+00:00   en           mortgage  \n",
       "...                          ...  ...                ...  \n",
       "69838  2022-12-06 11:31:54+00:00   en  wealth management  \n",
       "69839  2022-12-06 11:28:58+00:00   en  wealth management  \n",
       "69840  2022-12-06 11:15:56+00:00   en  wealth management  \n",
       "69841  2022-12-06 10:53:41+00:00   en  wealth management  \n",
       "69842  2022-12-06 10:53:39+00:00   en  wealth management  \n",
       "\n",
       "[69843 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweet_df = pd.read_csv(\"collected_data.csv\")\n",
    "tweet_df_copy = tweet_df\n",
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameym\\AppData\\Local\\Temp\\ipykernel_10008\\2030783118.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweet_df['text'][_] = tweet.text.encode('ascii', 'ignore').decode('ascii')\n"
     ]
    }
   ],
   "source": [
    "for _, tweet in tweet_df.iterrows(): \n",
    "    tweet_df['text'][_] = tweet.text.encode('ascii', 'ignore').decode('ascii') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>search_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1602612821585694721</td>\n",
       "      <td>1580423829801009152</td>\n",
       "      <td>@AltMediaWatch @SmilyngAssassin @Tugboat882191...</td>\n",
       "      <td>2022-12-13 10:34:06+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1602612794360307714</td>\n",
       "      <td>1425092150975401996</td>\n",
       "      <td>@RalphNeale1 @CTVNews Trudeau had *nothing* to...</td>\n",
       "      <td>2022-12-13 10:33:59+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1602612758645805056</td>\n",
       "      <td>1556214967573118980</td>\n",
       "      <td>BREAKING Jag Singer threatens to give up pensi...</td>\n",
       "      <td>2022-12-13 10:33:51+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1602612676168798209</td>\n",
       "      <td>703144956</td>\n",
       "      <td>@wetthroatbabyy @nojumper Hes been paying a mo...</td>\n",
       "      <td>2022-12-13 10:33:31+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1602612625904394240</td>\n",
       "      <td>1425092150975401996</td>\n",
       "      <td>@khumphries1 @CTVNews @JustinTrudeau Trudeau h...</td>\n",
       "      <td>2022-12-13 10:33:19+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69838</th>\n",
       "      <td>69838</td>\n",
       "      <td>1600090654167687168</td>\n",
       "      <td>83786013</td>\n",
       "      <td>Job Japan Private Banker Wealth Management. 35...</td>\n",
       "      <td>2022-12-06 11:31:54+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>wealth management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69839</th>\n",
       "      <td>69839</td>\n",
       "      <td>1600089915387219968</td>\n",
       "      <td>3290091422</td>\n",
       "      <td>From Wealth Management Professional to Career ...</td>\n",
       "      <td>2022-12-06 11:28:58+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>wealth management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69840</th>\n",
       "      <td>69840</td>\n",
       "      <td>1600086634665431041</td>\n",
       "      <td>1587324685414322177</td>\n",
       "      <td>@WakeOfWeek @JayRocha82 @RobSchneider @Yankees...</td>\n",
       "      <td>2022-12-06 11:15:56+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>wealth management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69841</th>\n",
       "      <td>69841</td>\n",
       "      <td>1600081033373827073</td>\n",
       "      <td>993891818998771719</td>\n",
       "      <td>BMO Wealth Management Names NewLeadership http...</td>\n",
       "      <td>2022-12-06 10:53:41+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>wealth management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69842</th>\n",
       "      <td>69842</td>\n",
       "      <td>1600081028063821824</td>\n",
       "      <td>1117356436437315585</td>\n",
       "      <td>BMO Wealth Management Names NewLeadership http...</td>\n",
       "      <td>2022-12-06 10:53:39+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>wealth management</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69843 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                   id            author_id  \\\n",
       "0               0  1602612821585694721  1580423829801009152   \n",
       "1               1  1602612794360307714  1425092150975401996   \n",
       "2               2  1602612758645805056  1556214967573118980   \n",
       "3               3  1602612676168798209            703144956   \n",
       "4               4  1602612625904394240  1425092150975401996   \n",
       "...           ...                  ...                  ...   \n",
       "69838       69838  1600090654167687168             83786013   \n",
       "69839       69839  1600089915387219968           3290091422   \n",
       "69840       69840  1600086634665431041  1587324685414322177   \n",
       "69841       69841  1600081033373827073   993891818998771719   \n",
       "69842       69842  1600081028063821824  1117356436437315585   \n",
       "\n",
       "                                                    text  \\\n",
       "0      @AltMediaWatch @SmilyngAssassin @Tugboat882191...   \n",
       "1      @RalphNeale1 @CTVNews Trudeau had *nothing* to...   \n",
       "2      BREAKING Jag Singer threatens to give up pensi...   \n",
       "3      @wetthroatbabyy @nojumper Hes been paying a mo...   \n",
       "4      @khumphries1 @CTVNews @JustinTrudeau Trudeau h...   \n",
       "...                                                  ...   \n",
       "69838  Job Japan Private Banker Wealth Management. 35...   \n",
       "69839  From Wealth Management Professional to Career ...   \n",
       "69840  @WakeOfWeek @JayRocha82 @RobSchneider @Yankees...   \n",
       "69841  BMO Wealth Management Names NewLeadership http...   \n",
       "69842  BMO Wealth Management Names NewLeadership http...   \n",
       "\n",
       "                            date lang        search_term  \n",
       "0      2022-12-13 10:34:06+00:00   en           mortgage  \n",
       "1      2022-12-13 10:33:59+00:00   en           mortgage  \n",
       "2      2022-12-13 10:33:51+00:00   en           mortgage  \n",
       "3      2022-12-13 10:33:31+00:00   en           mortgage  \n",
       "4      2022-12-13 10:33:19+00:00   en           mortgage  \n",
       "...                          ...  ...                ...  \n",
       "69838  2022-12-06 11:31:54+00:00   en  wealth management  \n",
       "69839  2022-12-06 11:28:58+00:00   en  wealth management  \n",
       "69840  2022-12-06 11:15:56+00:00   en  wealth management  \n",
       "69841  2022-12-06 10:53:41+00:00   en  wealth management  \n",
       "69842  2022-12-06 10:53:39+00:00   en  wealth management  \n",
       "\n",
       "[69843 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.to_csv(\"data_with_search_term.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_df_all = tweet_df\n",
    "tweet_df_all = tweet_df_all[tweet_df_all['text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69843, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>search_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1602612821585694721</td>\n",
       "      <td>1580423829801009152</td>\n",
       "      <td>@AltMediaWatch @SmilyngAssassin @Tugboat882191...</td>\n",
       "      <td>2022-12-13 10:34:06+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1602612794360307714</td>\n",
       "      <td>1425092150975401996</td>\n",
       "      <td>@RalphNeale1 @CTVNews Trudeau had *nothing* to...</td>\n",
       "      <td>2022-12-13 10:33:59+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1602612758645805056</td>\n",
       "      <td>1556214967573118980</td>\n",
       "      <td>BREAKING Jag Singer threatens to give up pensi...</td>\n",
       "      <td>2022-12-13 10:33:51+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1602612676168798209</td>\n",
       "      <td>703144956</td>\n",
       "      <td>@wetthroatbabyy @nojumper Hes been paying a mo...</td>\n",
       "      <td>2022-12-13 10:33:31+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1602612625904394240</td>\n",
       "      <td>1425092150975401996</td>\n",
       "      <td>@khumphries1 @CTVNews @JustinTrudeau Trudeau h...</td>\n",
       "      <td>2022-12-13 10:33:19+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   id            author_id  \\\n",
       "0           0  1602612821585694721  1580423829801009152   \n",
       "1           1  1602612794360307714  1425092150975401996   \n",
       "2           2  1602612758645805056  1556214967573118980   \n",
       "3           3  1602612676168798209            703144956   \n",
       "4           4  1602612625904394240  1425092150975401996   \n",
       "\n",
       "                                                text  \\\n",
       "0  @AltMediaWatch @SmilyngAssassin @Tugboat882191...   \n",
       "1  @RalphNeale1 @CTVNews Trudeau had *nothing* to...   \n",
       "2  BREAKING Jag Singer threatens to give up pensi...   \n",
       "3  @wetthroatbabyy @nojumper Hes been paying a mo...   \n",
       "4  @khumphries1 @CTVNews @JustinTrudeau Trudeau h...   \n",
       "\n",
       "                        date lang search_term  \n",
       "0  2022-12-13 10:34:06+00:00   en    mortgage  \n",
       "1  2022-12-13 10:33:59+00:00   en    mortgage  \n",
       "2  2022-12-13 10:33:51+00:00   en    mortgage  \n",
       "3  2022-12-13 10:33:31+00:00   en    mortgage  \n",
       "4  2022-12-13 10:33:19+00:00   en    mortgage  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tweet_df_all.shape);tweet_df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        @AltMediaWatch @SmilyngAssassin @Tugboat882191...\n",
       "1        @RalphNeale1 @CTVNews Trudeau had *nothing* to...\n",
       "2        BREAKING Jag Singer threatens to give up pensi...\n",
       "3        @wetthroatbabyy @nojumper Hes been paying a mo...\n",
       "4        @khumphries1 @CTVNews @JustinTrudeau Trudeau h...\n",
       "                               ...                        \n",
       "69838    Job Japan Private Banker Wealth Management. 35...\n",
       "69839    From Wealth Management Professional to Career ...\n",
       "69840    @WakeOfWeek @JayRocha82 @RobSchneider @Yankees...\n",
       "69841    BMO Wealth Management Names NewLeadership http...\n",
       "69842    BMO Wealth Management Names NewLeadership http...\n",
       "Name: text, Length: 69843, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_all['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>search_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1602612821585694721</td>\n",
       "      <td>1580423829801009152</td>\n",
       "      <td>@AltMediaWatch @SmilyngAssassin @Tugboat882191...</td>\n",
       "      <td>2022-12-13 10:34:06+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1602612794360307714</td>\n",
       "      <td>1425092150975401996</td>\n",
       "      <td>@RalphNeale1 @CTVNews Trudeau had *nothing* to...</td>\n",
       "      <td>2022-12-13 10:33:59+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1602612758645805056</td>\n",
       "      <td>1556214967573118980</td>\n",
       "      <td>BREAKING Jag Singer threatens to give up pensi...</td>\n",
       "      <td>2022-12-13 10:33:51+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1602612676168798209</td>\n",
       "      <td>703144956</td>\n",
       "      <td>@wetthroatbabyy @nojumper Hes been paying a mo...</td>\n",
       "      <td>2022-12-13 10:33:31+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1602612625904394240</td>\n",
       "      <td>1425092150975401996</td>\n",
       "      <td>@khumphries1 @CTVNews @JustinTrudeau Trudeau h...</td>\n",
       "      <td>2022-12-13 10:33:19+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69838</th>\n",
       "      <td>69838</td>\n",
       "      <td>1600090654167687168</td>\n",
       "      <td>83786013</td>\n",
       "      <td>Job Japan Private Banker Wealth Management. 35...</td>\n",
       "      <td>2022-12-06 11:31:54+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>wealth management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69839</th>\n",
       "      <td>69839</td>\n",
       "      <td>1600089915387219968</td>\n",
       "      <td>3290091422</td>\n",
       "      <td>From Wealth Management Professional to Career ...</td>\n",
       "      <td>2022-12-06 11:28:58+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>wealth management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69840</th>\n",
       "      <td>69840</td>\n",
       "      <td>1600086634665431041</td>\n",
       "      <td>1587324685414322177</td>\n",
       "      <td>@WakeOfWeek @JayRocha82 @RobSchneider @Yankees...</td>\n",
       "      <td>2022-12-06 11:15:56+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>wealth management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69841</th>\n",
       "      <td>69841</td>\n",
       "      <td>1600081033373827073</td>\n",
       "      <td>993891818998771719</td>\n",
       "      <td>BMO Wealth Management Names NewLeadership http...</td>\n",
       "      <td>2022-12-06 10:53:41+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>wealth management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69842</th>\n",
       "      <td>69842</td>\n",
       "      <td>1600081028063821824</td>\n",
       "      <td>1117356436437315585</td>\n",
       "      <td>BMO Wealth Management Names NewLeadership http...</td>\n",
       "      <td>2022-12-06 10:53:39+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>wealth management</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69843 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                   id            author_id  \\\n",
       "0               0  1602612821585694721  1580423829801009152   \n",
       "1               1  1602612794360307714  1425092150975401996   \n",
       "2               2  1602612758645805056  1556214967573118980   \n",
       "3               3  1602612676168798209            703144956   \n",
       "4               4  1602612625904394240  1425092150975401996   \n",
       "...           ...                  ...                  ...   \n",
       "69838       69838  1600090654167687168             83786013   \n",
       "69839       69839  1600089915387219968           3290091422   \n",
       "69840       69840  1600086634665431041  1587324685414322177   \n",
       "69841       69841  1600081033373827073   993891818998771719   \n",
       "69842       69842  1600081028063821824  1117356436437315585   \n",
       "\n",
       "                                                    text  \\\n",
       "0      @AltMediaWatch @SmilyngAssassin @Tugboat882191...   \n",
       "1      @RalphNeale1 @CTVNews Trudeau had *nothing* to...   \n",
       "2      BREAKING Jag Singer threatens to give up pensi...   \n",
       "3      @wetthroatbabyy @nojumper Hes been paying a mo...   \n",
       "4      @khumphries1 @CTVNews @JustinTrudeau Trudeau h...   \n",
       "...                                                  ...   \n",
       "69838  Job Japan Private Banker Wealth Management. 35...   \n",
       "69839  From Wealth Management Professional to Career ...   \n",
       "69840  @WakeOfWeek @JayRocha82 @RobSchneider @Yankees...   \n",
       "69841  BMO Wealth Management Names NewLeadership http...   \n",
       "69842  BMO Wealth Management Names NewLeadership http...   \n",
       "\n",
       "                            date lang        search_term  \n",
       "0      2022-12-13 10:34:06+00:00   en           mortgage  \n",
       "1      2022-12-13 10:33:59+00:00   en           mortgage  \n",
       "2      2022-12-13 10:33:51+00:00   en           mortgage  \n",
       "3      2022-12-13 10:33:31+00:00   en           mortgage  \n",
       "4      2022-12-13 10:33:19+00:00   en           mortgage  \n",
       "...                          ...  ...                ...  \n",
       "69838  2022-12-06 11:31:54+00:00   en  wealth management  \n",
       "69839  2022-12-06 11:28:58+00:00   en  wealth management  \n",
       "69840  2022-12-06 11:15:56+00:00   en  wealth management  \n",
       "69841  2022-12-06 10:53:41+00:00   en  wealth management  \n",
       "69842  2022-12-06 10:53:39+00:00   en  wealth management  \n",
       "\n",
       "[69843 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_comp = tweet_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "search_term\n",
       "credit card          9817\n",
       "current account      3686\n",
       "insurance            9872\n",
       "investment           9987\n",
       "money transfer       7246\n",
       "mortgage             9964\n",
       "pension              9972\n",
       "personal loan        1440\n",
       "savings account      4535\n",
       "tax advice            641\n",
       "wealth management    2683\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_comp.groupby('search_term')['id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Topic Extraction with LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove unnessary words\n",
    "#Complie all regular expressions\n",
    "isURL = re.compile(r'http[s]?:// (?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', re.VERBOSE | re.IGNORECASE)\n",
    "isRTusername = re.compile(r'^RT+[\\s]+(@[\\w_]+:)',re.VERBOSE | re.IGNORECASE) #r'^RT+[\\s]+(@[\\w_]+:)'\n",
    "isEntity = re.compile(r'@[\\w_]+', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "# Helper functions\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]])) \n",
    "\n",
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer, lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "     \n",
    "        \n",
    "def clean_tweet(row):\n",
    "    row = isURL.sub(\"\",row)\n",
    "    row = isRTusername.sub(\"\",row)\n",
    "    row = isEntity.sub(\"\",row)\n",
    "    row = re.sub('[#|\\n|\\t]+', '', row)\n",
    "    return row\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in tok.sent_tokenize(text) for word in tok.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameym\\AppData\\Local\\Temp\\ipykernel_10008\\1323273592.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tweet_df_comp['text_clean'] = tweet_df_comp['text_clean'].str.replace(RE_PUNCTUATION, \"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>search_term</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1602612821585694721</td>\n",
       "      <td>1580423829801009152</td>\n",
       "      <td>@AltMediaWatch @SmilyngAssassin @Tugboat882191...</td>\n",
       "      <td>2022-12-13 10:34:06+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>No record and a juicy false accusation cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1602612794360307714</td>\n",
       "      <td>1425092150975401996</td>\n",
       "      <td>@RalphNeale1 @CTVNews Trudeau had *nothing* to...</td>\n",
       "      <td>2022-12-13 10:33:59+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>Trudeau had nothing to do with mortgage rate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1602612758645805056</td>\n",
       "      <td>1556214967573118980</td>\n",
       "      <td>BREAKING Jag Singer threatens to give up pensi...</td>\n",
       "      <td>2022-12-13 10:33:51+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>BREAKING Jag Singer threatens to give up pensi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1602612676168798209</td>\n",
       "      <td>703144956</td>\n",
       "      <td>@wetthroatbabyy @nojumper Hes been paying a mo...</td>\n",
       "      <td>2022-12-13 10:33:31+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>Hes been paying a mortgage for 12 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1602612625904394240</td>\n",
       "      <td>1425092150975401996</td>\n",
       "      <td>@khumphries1 @CTVNews @JustinTrudeau Trudeau h...</td>\n",
       "      <td>2022-12-13 10:33:19+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>Trudeau had nothing to do with mortgage rat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   id            author_id  \\\n",
       "0           0  1602612821585694721  1580423829801009152   \n",
       "1           1  1602612794360307714  1425092150975401996   \n",
       "2           2  1602612758645805056  1556214967573118980   \n",
       "3           3  1602612676168798209            703144956   \n",
       "4           4  1602612625904394240  1425092150975401996   \n",
       "\n",
       "                                                text  \\\n",
       "0  @AltMediaWatch @SmilyngAssassin @Tugboat882191...   \n",
       "1  @RalphNeale1 @CTVNews Trudeau had *nothing* to...   \n",
       "2  BREAKING Jag Singer threatens to give up pensi...   \n",
       "3  @wetthroatbabyy @nojumper Hes been paying a mo...   \n",
       "4  @khumphries1 @CTVNews @JustinTrudeau Trudeau h...   \n",
       "\n",
       "                        date lang search_term  \\\n",
       "0  2022-12-13 10:34:06+00:00   en    mortgage   \n",
       "1  2022-12-13 10:33:59+00:00   en    mortgage   \n",
       "2  2022-12-13 10:33:51+00:00   en    mortgage   \n",
       "3  2022-12-13 10:33:31+00:00   en    mortgage   \n",
       "4  2022-12-13 10:33:19+00:00   en    mortgage   \n",
       "\n",
       "                                          text_clean  \n",
       "0      No record and a juicy false accusation cla...  \n",
       "1    Trudeau had nothing to do with mortgage rate...  \n",
       "2  BREAKING Jag Singer threatens to give up pensi...  \n",
       "3            Hes been paying a mortgage for 12 years  \n",
       "4     Trudeau had nothing to do with mortgage rat...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove urls and retweets and entities from the text\n",
    "tweet_df_comp['text_clean'] = tweet_df_comp['text'].apply(lambda row:clean_tweet(row))\n",
    "\n",
    "#remove punctuations\n",
    "RE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])  \n",
    "tweet_df_comp['text_clean'] = tweet_df_comp['text_clean'].str.replace(RE_PUNCTUATION, \"\")\n",
    "tweet_df_comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of stopwords\n",
    "stop_words= stopwords.words('english') #import stopwords from NLTK package\n",
    "readInStopwords = pd.read_csv(\"./twitter-analytics/pre_process/twitterStopWords.csv\", encoding='ISO-8859-1') # import stopwords from CSV file as pandas data frame\n",
    "readInStopwords = readInStopwords.wordList.tolist() # convert pandas data frame to a list\n",
    "readInStopwords.append('http')\n",
    "readInStopwords.append('https')\n",
    "readInStopwords.extend(search_terms)\n",
    "\n",
    "stop_list = stop_words + readInStopwords\n",
    "stop_list = list(set(stop_list)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameter for lda, i am selecrign 3 topic and 4 words for each of the search terms \n",
    "number_topics = 5\n",
    "number_words = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mortgage', 'current account', 'savings account', 'insurance',\n",
       "       'credit card', 'pension', 'personal loan', 'money transfer',\n",
       "       'tax advice', 'investment', 'wealth management'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_comp['search_term'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mortgage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'money', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'transfer', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "     index    Word 0     Word 1     Word 2   Word 3      Word 4  topic_index\n",
      "0  Topic 0     deals        amp         us      see       rates            0\n",
      "1  Topic 1       pay      rates       home     like       house            1\n",
      "2  Topic 2       pay       rent      rates      get      people            2\n",
      "3  Topic 3     years      rates  available  maximum     checked            3\n",
      "4  Topic 4  received  responses      years   latest  homebuyers            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\ameym\\AppData\\Local\\Temp\\ipykernel_10008\\496991535.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_all_topics = tweets_all_topics.append(tweets_topics_words)\n",
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'money', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'transfer', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current account\n",
      "Topics found via LDA:\n",
      "     index   Word 0   Word 1   Word 2     Word 3    Word 4  topic_index\n",
      "0  Topic 0    start    shows     prev  investing     chart            0\n",
      "1  Topic 1      buy     take    build  community  benefits            1\n",
      "2  Topic 2       us   please     know       help        hi            2\n",
      "3  Topic 3     bank  deficit  billion        one     still            3\n",
      "4  Topic 4  twitter   people      one         im      dont            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\ameym\\AppData\\Local\\Temp\\ipykernel_10008\\496991535.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_all_topics = tweets_all_topics.append(tweets_topics_words)\n",
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'money', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'transfer', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "savings account\n",
      "Topics found via LDA:\n",
      "     index    Word 0  Word 1    Word 2     Word 3    Word 4  topic_index\n",
      "0  Topic 0      bank     get  interest      money     yotta            0\n",
      "1  Topic 1    saving  others     daily  deposited      join            1\n",
      "2  Topic 2     money     pay  interest        get  checking            2\n",
      "3  Topic 3     money    bank      dont     credit      card            3\n",
      "4  Topic 4  interest    rate     money     health       tax            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\ameym\\AppData\\Local\\Temp\\ipykernel_10008\\496991535.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_all_topics = tweets_all_topics.append(tweets_topics_words)\n",
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'money', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'transfer', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insurance\n",
      "Topics found via LDA:\n",
      "     index  Word 0    Word 1  Word 2  Word 3     Word 4  topic_index\n",
      "0  Topic 0  health       pay   month     get        car            0\n",
      "1  Topic 1     new  business  agency  person     search            1\n",
      "2  Topic 2  health       get    dont     car     people            2\n",
      "3  Topic 3     get      like   money     pay  companies            3\n",
      "4  Topic 4     car       new    life     pay      rates            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\ameym\\AppData\\Local\\Temp\\ipykernel_10008\\496991535.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_all_topics = tweets_all_topics.append(tweets_topics_words)\n",
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'money', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'transfer', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credit card\n",
      "Topics found via LDA:\n",
      "     index  Word 0   Word 1  Word 2       Word 3    Word 4  topic_index\n",
      "0  Topic 0  credit      buy     amp         team       new            0\n",
      "1  Topic 1  credit      pay      im         dont       get            1\n",
      "2  Topic 2  please  support  handle     official  mobikwik            2\n",
      "3  Topic 3  credit  account     pay       number      need            3\n",
      "4  Topic 4  credit     bank   debit  application    please            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\ameym\\AppData\\Local\\Temp\\ipykernel_10008\\496991535.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_all_topics = tweets_all_topics.append(tweets_topics_words)\n",
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'money', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'transfer', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pension\n",
      "Topics found via LDA:\n",
      "     index Word 0  Word 1  Word 2     Word 3      Word 4  topic_index\n",
      "0  Topic 0    old  scheme    govt  employees  government            0\n",
      "1  Topic 1  funds     amp  robbed      years         get            1\n",
      "2  Topic 2  money     get   funds       dont        plan            2\n",
      "3  Topic 3    pay     get  sector     public     private            3\n",
      "4  Topic 4  funds     get   state       work       years            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\ameym\\AppData\\Local\\Temp\\ipykernel_10008\\496991535.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_all_topics = tweets_all_topics.append(tweets_topics_words)\n",
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'money', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'transfer', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personal loan\n",
      "Topics found via LDA:\n",
      "     index    Word 0    Word 1   Word 2     Word 3     Word 4  topic_index\n",
      "0  Topic 0      loan  personal  student  repayment       year            0\n",
      "1  Topic 1      loan  personal    loans        get        pay            1\n",
      "2  Topic 2  personal      loan    loans    problem  financial            2\n",
      "3  Topic 3      loan  personal     bank     credit       home            3\n",
      "4  Topic 4      loan  personal      get   interest       need            4\n",
      "money transfer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\ameym\\AppData\\Local\\Temp\\ipykernel_10008\\496991535.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_all_topics = tweets_all_topics.append(tweets_topics_words)\n",
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'money', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'transfer', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "     index    Word 0     Word 1  Word 2  Word 3   Word 4  topic_index\n",
      "0  Topic 0      send       land     get   world  project            0\n",
      "1  Topic 1    wealth       make     amp    dont      use            1\n",
      "2  Topic 2   account       bank    need  please      get            2\n",
      "3  Topic 3  trillion  unlimited  rupiah    rank  country            3\n",
      "4  Topic 4    crypto         go  people    cash     like            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\ameym\\AppData\\Local\\Temp\\ipykernel_10008\\496991535.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_all_topics = tweets_all_topics.append(tweets_topics_words)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tax advice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'money', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'transfer', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "     index  Word 0     Word 1 Word 2 Word 3 Word 4  topic_index\n",
      "0  Topic 0  advice      money   dont   help    get            0\n",
      "1  Topic 1  advice       need  check    via   blog            1\n",
      "2  Topic 2  advice   business  legal   dont    get            2\n",
      "3  Topic 3  advice     people    pay    get   good            3\n",
      "4  Topic 4  advice  financial  money    get     us            4\n",
      "investment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\ameym\\AppData\\Local\\Temp\\ipykernel_10008\\496991535.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_all_topics = tweets_all_topics.append(tweets_topics_words)\n",
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'money', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'transfer', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "     index       Word 0   Word 1       Word 2  Word 3   Word 4  topic_index\n",
      "0  Topic 0       crypto  program       little   group     join            0\n",
      "1  Topic 1    following       md  instruction      x3  telegrm            1\n",
      "2  Topic 2        money      new         dont     amp     like            2\n",
      "3  Topic 3         link    click         capo    free   little            3\n",
      "4  Topic 4  opportunity     come        worth  crypto    world            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\ameym\\AppData\\Local\\Temp\\ipykernel_10008\\496991535.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_all_topics = tweets_all_topics.append(tweets_topics_words)\n",
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'money', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'transfer', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wealth management\n",
      "Topics found via LDA:\n",
      "     index  Word 0      Word 1 Word 2     Word 3     Word 4  topic_index\n",
      "0  Topic 0  wealth  management     us      banks  announced            0\n",
      "1  Topic 1    read    economic     us     stocks    remains            1\n",
      "2  Topic 2  wealth  management   help  financial    article            2\n",
      "3  Topic 3  report        read  ahead         us       move            3\n",
      "4  Topic 4  wealth  management    one      offer    leading            4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\ameym\\AppData\\Local\\Temp\\ipykernel_10008\\496991535.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_all_topics = tweets_all_topics.append(tweets_topics_words)\n"
     ]
    }
   ],
   "source": [
    "tweets_all_topics= pd.DataFrame()\n",
    "# term frequency modelling\n",
    "for terms in tweet_df_comp['search_term'].unique():\n",
    "    print(terms)\n",
    "    tweets_search_topics  = tweet_df_comp[tweet_df_comp['search_term']==terms].reset_index(drop=True)\n",
    "    corpus = tweets_search_topics['text_clean'].tolist()\n",
    "    # print(corpus)\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.9, min_df=0.00, stop_words=stop_list, tokenizer=tokenize_only) # Use tf (raw term count) features for LDA.\n",
    "    tf = tf_vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # Create and fit the LDA model\n",
    "    model = LDA(n_components=number_topics, n_jobs=-1)\n",
    "    id_topic = model.fit(tf)\n",
    "    # Print the topics found by the LDA model\n",
    "    print(\"Topics found via LDA:\")\n",
    "    topic_keywords = show_topics(vectorizer=tf_vectorizer, lda_model=model, n_words=number_words)        \n",
    "    # Topic - Keywords Dataframe\n",
    "    df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "    df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "    df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "    df_topic_keywords = df_topic_keywords.reset_index()\n",
    "    df_topic_keywords['topic_index'] = df_topic_keywords['index'].str.split(' ', n = 1, expand = True)[[1]].astype('int')\n",
    "    print(df_topic_keywords)\n",
    "    \n",
    "    ############ get the dominat topic for each document in a data frame ###############\n",
    "    # Create Document — Topic Matrix\n",
    "    lda_output = model.transform(tf)\n",
    "    # column names\n",
    "    topicnames = [\"Topic\" + str(i) for i in range(model.n_components)]\n",
    "    # index names\n",
    "    docnames = [\"Doc\" + str(i) for i in range(len(corpus))]\n",
    "    \n",
    "    # Make the pandas dataframe\n",
    "    df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "    # Get dominant topic for each document\n",
    "    dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "    df_document_topic['dominant_topic'] = dominant_topic   \n",
    "    df_document_topic = df_document_topic.reset_index()\n",
    "        \n",
    "    #combine all the search terms into one data frame\n",
    "    tweets_topics = tweets_search_topics.merge(df_document_topic, left_index=True, right_index=True, how='left')\n",
    "    tweets_topics_words = tweets_topics.merge(df_topic_keywords, how='left', left_on='dominant_topic', right_on='topic_index')\n",
    "    tweets_all_topics = tweets_all_topics.append(tweets_topics_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69843, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>search_term</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>index_x</th>\n",
       "      <th>Topic0</th>\n",
       "      <th>...</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>index_y</th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>topic_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1602612821585694721</td>\n",
       "      <td>1580423829801009152</td>\n",
       "      <td>@AltMediaWatch @SmilyngAssassin @Tugboat882191...</td>\n",
       "      <td>2022-12-13 10:34:06+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>No record and a juicy false accusation cla...</td>\n",
       "      <td>Doc0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1</td>\n",
       "      <td>Topic 1</td>\n",
       "      <td>pay</td>\n",
       "      <td>rates</td>\n",
       "      <td>home</td>\n",
       "      <td>like</td>\n",
       "      <td>house</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1602612794360307714</td>\n",
       "      <td>1425092150975401996</td>\n",
       "      <td>@RalphNeale1 @CTVNews Trudeau had *nothing* to...</td>\n",
       "      <td>2022-12-13 10:33:59+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>Trudeau had nothing to do with mortgage rate...</td>\n",
       "      <td>Doc1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1</td>\n",
       "      <td>Topic 1</td>\n",
       "      <td>pay</td>\n",
       "      <td>rates</td>\n",
       "      <td>home</td>\n",
       "      <td>like</td>\n",
       "      <td>house</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1602612758645805056</td>\n",
       "      <td>1556214967573118980</td>\n",
       "      <td>BREAKING Jag Singer threatens to give up pensi...</td>\n",
       "      <td>2022-12-13 10:33:51+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>BREAKING Jag Singer threatens to give up pensi...</td>\n",
       "      <td>Doc2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>Topic 1</td>\n",
       "      <td>pay</td>\n",
       "      <td>rates</td>\n",
       "      <td>home</td>\n",
       "      <td>like</td>\n",
       "      <td>house</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1602612676168798209</td>\n",
       "      <td>703144956</td>\n",
       "      <td>@wetthroatbabyy @nojumper Hes been paying a mo...</td>\n",
       "      <td>2022-12-13 10:33:31+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>Hes been paying a mortgage for 12 years</td>\n",
       "      <td>Doc3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>Topic 1</td>\n",
       "      <td>pay</td>\n",
       "      <td>rates</td>\n",
       "      <td>home</td>\n",
       "      <td>like</td>\n",
       "      <td>house</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1602612625904394240</td>\n",
       "      <td>1425092150975401996</td>\n",
       "      <td>@khumphries1 @CTVNews @JustinTrudeau Trudeau h...</td>\n",
       "      <td>2022-12-13 10:33:19+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>Trudeau had nothing to do with mortgage rat...</td>\n",
       "      <td>Doc4</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3</td>\n",
       "      <td>Topic 3</td>\n",
       "      <td>years</td>\n",
       "      <td>rates</td>\n",
       "      <td>available</td>\n",
       "      <td>maximum</td>\n",
       "      <td>checked</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   id            author_id  \\\n",
       "0           0  1602612821585694721  1580423829801009152   \n",
       "1           1  1602612794360307714  1425092150975401996   \n",
       "2           2  1602612758645805056  1556214967573118980   \n",
       "3           3  1602612676168798209            703144956   \n",
       "4           4  1602612625904394240  1425092150975401996   \n",
       "\n",
       "                                                text  \\\n",
       "0  @AltMediaWatch @SmilyngAssassin @Tugboat882191...   \n",
       "1  @RalphNeale1 @CTVNews Trudeau had *nothing* to...   \n",
       "2  BREAKING Jag Singer threatens to give up pensi...   \n",
       "3  @wetthroatbabyy @nojumper Hes been paying a mo...   \n",
       "4  @khumphries1 @CTVNews @JustinTrudeau Trudeau h...   \n",
       "\n",
       "                        date lang search_term  \\\n",
       "0  2022-12-13 10:34:06+00:00   en    mortgage   \n",
       "1  2022-12-13 10:33:59+00:00   en    mortgage   \n",
       "2  2022-12-13 10:33:51+00:00   en    mortgage   \n",
       "3  2022-12-13 10:33:31+00:00   en    mortgage   \n",
       "4  2022-12-13 10:33:19+00:00   en    mortgage   \n",
       "\n",
       "                                          text_clean index_x  Topic0  ...  \\\n",
       "0      No record and a juicy false accusation cla...    Doc0    0.26  ...   \n",
       "1    Trudeau had nothing to do with mortgage rate...    Doc1    0.03  ...   \n",
       "2  BREAKING Jag Singer threatens to give up pensi...    Doc2    0.01  ...   \n",
       "3            Hes been paying a mortgage for 12 years    Doc3    0.05  ...   \n",
       "4     Trudeau had nothing to do with mortgage rat...    Doc4    0.03  ...   \n",
       "\n",
       "   Topic3  Topic4  dominant_topic  index_y  Word 0 Word 1     Word 2   Word 3  \\\n",
       "0    0.02    0.02               1  Topic 1     pay  rates       home     like   \n",
       "1    0.27    0.03               1  Topic 1     pay  rates       home     like   \n",
       "2    0.01    0.01               1  Topic 1     pay  rates       home     like   \n",
       "3    0.30    0.05               1  Topic 1     pay  rates       home     like   \n",
       "4    0.53    0.03               3  Topic 3   years  rates  available  maximum   \n",
       "\n",
       "    Word 4 topic_index  \n",
       "0    house           1  \n",
       "1    house           1  \n",
       "2    house           1  \n",
       "3    house           1  \n",
       "4  checked           3  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_all_topics = tweets_all_topics.reset_index(drop=True)\n",
    "print(tweets_all_topics.shape)\n",
    "tweets_all_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_all_topics.to_csv('./processed_data/tweets_all_topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_all_topics = pd.read_csv('./processed_data/tweets_all_topics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Sentiment analysis using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import h5py\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_16 (Embedding)    (None, 56, 300)           120000300 \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 256)              439296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120,576,310\n",
      "Trainable params: 576,010\n",
      "Non-trainable params: 120,000,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# read in the weight of the trained model.\n",
    "weight_path = './twitter-analytics//models/dl_sentiment_model/best_weight_glove_bi_512.hdf5'\n",
    "prd_model = load_model(weight_path)\n",
    "prd_model.summary()\n",
    "word_idx = json.load(open(\"./twitter-analytics//models/dl_sentiment_model/word_idx.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment_DL(prd_model, text_data, word_idx):\n",
    "\n",
    "    #data = \"Pass the salt\"\n",
    "\n",
    "    live_list = []\n",
    "    batchSize = len(text_data)\n",
    "    live_list_np = np.zeros((56,batchSize))\n",
    "    for index, row in text_data.iterrows():\n",
    "        #print (index)\n",
    "        text_data_sample = text_data['text_clean'][index]\n",
    "        # split the sentence into its words and remove any punctuations.\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        text_data_list = tokenizer.tokenize(text_data_sample)\n",
    "\n",
    "        #text_data_list = text_data_sample.split()\n",
    "\n",
    "\n",
    "        labels = np.array(['1','2','3','4','5','6','7','8','9','10'], dtype = \"int\")\n",
    "        #word_idx['I']\n",
    "        # get index for the live stage\n",
    "        data_index = np.array([word_idx[word.lower()] if word.lower() in word_idx else 0 for word in text_data_list])\n",
    "        data_index_np = np.array(data_index)\n",
    "\n",
    "        # padded with zeros of length 56 i.e maximum length\n",
    "        padded_array = np.zeros(56)\n",
    "        padded_array[:data_index_np.shape[0]] = data_index_np[:56]\n",
    "        data_index_np_pad = padded_array.astype(int)\n",
    "\n",
    "\n",
    "        live_list.append(data_index_np_pad)\n",
    "\n",
    "    live_list_np = np.asarray(live_list)\n",
    "    score = prd_model.predict(live_list_np, batch_size=batchSize, verbose=0)\n",
    "    single_score = np.round(np.dot(score, labels)/10,decimals=2)\n",
    "\n",
    "    score_all  = []\n",
    "    for each_score in score:\n",
    "\n",
    "        top_3_index = np.argsort(each_score)[-3:]\n",
    "        top_3_scores = each_score[top_3_index]\n",
    "        top_3_weights = top_3_scores/np.sum(top_3_scores)\n",
    "        single_score_dot = np.round(np.dot(top_3_index, top_3_weights)/10, decimals = 2)\n",
    "        score_all.append(single_score_dot)\n",
    "\n",
    "    text_data['Sentiment_Score'] = pd.DataFrame(score_all)\n",
    "\n",
    "    return text_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data =  tweets_all_topics\n",
    "text_out = get_sentiment_DL(prd_model, text_data, word_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example of negative tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9152</th>\n",
       "      <td>@stinky_linke @RepsMasi @elonmusk 2) then I re...</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32546</th>\n",
       "      <td>@AmericanAir the worst airline ever! Im so dis...</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46035</th>\n",
       "      <td>@SaulStaniforth And again @wesstreeting showin...</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16480</th>\n",
       "      <td>THEY FUCKING FORGOT TO PUT MY VACATION ON MY P...</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38686</th>\n",
       "      <td>@UKurbanite @ClarkeMicah Absolute rubbish. It ...</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  Sentiment_Score\n",
       "9152   @stinky_linke @RepsMasi @elonmusk 2) then I re...             0.03\n",
       "32546  @AmericanAir the worst airline ever! Im so dis...             0.03\n",
       "46035  @SaulStaniforth And again @wesstreeting showin...             0.03\n",
       "16480  THEY FUCKING FORGOT TO PUT MY VACATION ON MY P...             0.03\n",
       "38686  @UKurbanite @ClarkeMicah Absolute rubbish. It ...             0.03"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_out.sort_values(by='Sentiment_Score')[['text','Sentiment_Score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example of positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20000</th>\n",
       "      <th>31755</th>\n",
       "      <th>58554</th>\n",
       "      <th>30168</th>\n",
       "      <th>64404</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>@ElieNYC Fabulous moments with the children. T...</td>\n",
       "      <td>I didnt have to throw any Xmas gifts on a cred...</td>\n",
       "      <td>@indiainpixels Vizag is a beautiful place with...</td>\n",
       "      <td>@Cameron_Steelh @nelo_musk @Gtwy @gregggonsalv...</td>\n",
       "      <td>@SAPoliceService Greatest desire of earning wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             20000  \\\n",
       "text             @ElieNYC Fabulous moments with the children. T...   \n",
       "Sentiment_Score                                               0.85   \n",
       "\n",
       "                                                             31755  \\\n",
       "text             I didnt have to throw any Xmas gifts on a cred...   \n",
       "Sentiment_Score                                               0.85   \n",
       "\n",
       "                                                             58554  \\\n",
       "text             @indiainpixels Vizag is a beautiful place with...   \n",
       "Sentiment_Score                                               0.84   \n",
       "\n",
       "                                                             30168  \\\n",
       "text             @Cameron_Steelh @nelo_musk @Gtwy @gregggonsalv...   \n",
       "Sentiment_Score                                               0.83   \n",
       "\n",
       "                                                             64404  \n",
       "text             @SAPoliceService Greatest desire of earning wi...  \n",
       "Sentiment_Score                                               0.82  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_out.sort_values(by='Sentiment_Score', ascending=False)[['text','Sentiment_Score']].head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1317210"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_out.query('0<= Sentiment_Score < 0.5').sort_values(by='Sentiment_Score', ascending=False).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289179"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_out.query('0.5<= Sentiment_Score').sort_values(by='Sentiment_Score', ascending=False).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the output files\n",
    "text_out.to_csv('./processed_data/tweets_topics_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_out = pd.read_csv(\"./processed_data/tweets_topics_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5:  Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below section is implementing a stanford 3 class NER tagger. The model is trained based on on supervised Conditional Random Field (CRF) model. Additional information on the model is available at https://nlp.stanford.edu/software/CRF-NER.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_NER(text_data):\n",
    "    stanford_classifier = './twitter-analytics//models/ner/english.all.3class.distsim.crf.ser.gz'\n",
    "    stanford_ner_path = './twitter-analytics//models/ner/stanford-ner.jar'\n",
    "\n",
    "    st = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')\n",
    "    print ('start get_NER')\n",
    "    text_out = text_data.copy()\n",
    "    doc = [ docs + ' 12345678 ' for docs in list(text_data['text'])]\n",
    "    # ------------------------- Stanford Named Entity Recognition\n",
    "    tokens = nltk.word_tokenize(str(doc))\n",
    "    entities = st.tag(tokens) # actual tagging takes place using Stanford NER algorithm\n",
    "\n",
    "\n",
    "    entities = [list(elem) for elem in entities] # Convert list of tuples to list of list\n",
    "    print ('tag complete')\n",
    "    for idx,element in enumerate(entities):\n",
    "        try:\n",
    "            if entities[idx][0] == '12345678':\n",
    "                entities[idx][1] = \"DOC_NUMBER\"  #  Modify data by adding the tag \"Doc_Number\"\n",
    "            elif entities[idx][1] == \"PERSON\" and entities[idx + 1][1] == \"PERSON\":\n",
    "                entities[idx + 1][0] = entities[idx][0] + '-' + entities[idx+1][0]\n",
    "                entities[idx][1] = 'Combined'\n",
    "            # Combine consecutive Organization names\n",
    "            elif entities[idx][1] == 'ORGANIZATION' and entities[idx + 1][1] == 'ORGANIZATION':\n",
    "                entities[idx + 1][0] = entities[idx][0] + '-' + entities[idx+1][0]\n",
    "                entities[idx][1] = 'Combined'\n",
    "        except IndexError:\n",
    "            break\n",
    "    print ('enumerate complete')\n",
    "    # Filter list of list for the words we are interested in\n",
    "    filter_list = ['DOC_NUMBER','PERSON','LOCATION','ORGANIZATION']\n",
    "    entityWordList = [element for element in entities if any(i in element for i in filter_list)]\n",
    "\n",
    "    entityString = ' '.join(str(word) for insideList in entityWordList for word in insideList) # convert list to string and concatenate it\n",
    "    entitySubString = entityString.split(\"DOC_NUMBER\") # split the string using the separator 'TWEET_NUMBER'\n",
    "    del entitySubString[-1] # delete the extra blank row created in the previous step\n",
    "\n",
    "    # Store the classified NERs in the main tweet data frame\n",
    "    for idx,docNER in enumerate(entitySubString):\n",
    "        docNER = docNER.strip().split() # split the string into word list\n",
    "        # Filter for words tagged as Organization and store it in data frame\n",
    "        text_out.loc[idx,'Organization'] =  ','.join([docNER[i-1]  for i,x in enumerate(docNER) if x == 'ORGANIZATION'])\n",
    "        # Filter for words tagged as LOCATION and store it in data frame\n",
    "        text_out.loc[idx,'Place'] = ','.join([docNER[i-1] for i,x in enumerate(docNER) if x == 'LOCATION'])\n",
    "        # Filter for words tagged as PERSON and store it in data frame\n",
    "        text_out.loc[idx,'Person'] = ','.join([docNER[i-1]  for i,x in enumerate(docNER) if x == 'PERSON'])\n",
    "\n",
    "    print ('process complete')\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1606389"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_out.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start get_NER\n",
      "tag complete\n",
      "enumerate complete\n",
      "process complete\n",
      "start get_NER\n",
      "tag complete\n",
      "enumerate complete\n",
      "process complete\n",
      "start get_NER\n",
      "tag complete\n",
      "enumerate complete\n",
      "process complete\n",
      "start get_NER\n",
      "tag complete\n",
      "enumerate complete\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [38], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m iters \u001b[39m=\u001b[39m  \u001b[39m100\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(iters):\n\u001b[1;32m----> 4\u001b[0m   text_ner_out[i] \u001b[39m=\u001b[39m get_NER(text_out\u001b[39m.\u001b[39;49miloc[i\u001b[39m*\u001b[39;49mmath\u001b[39m.\u001b[39;49mceil(text_out\u001b[39m.\u001b[39;49msize\u001b[39m/\u001b[39;49miters):(i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m*\u001b[39;49mmath\u001b[39m.\u001b[39;49mceil(text_out\u001b[39m.\u001b[39;49msize\u001b[39m/\u001b[39;49miters),:])\n",
      "Cell \u001b[1;32mIn [35], line 42\u001b[0m, in \u001b[0;36mget_NER\u001b[1;34m(text_data)\u001b[0m\n\u001b[0;32m     40\u001b[0m docNER \u001b[39m=\u001b[39m docNER\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit() \u001b[39m# split the string into word list\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m# Filter for words tagged as Organization and store it in data frame\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m text_out\u001b[39m.\u001b[39mloc[idx,\u001b[39m'\u001b[39m\u001b[39mOrganization\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m  \u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([docNER[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]  \u001b[39mfor\u001b[39;00m i,x \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(docNER) \u001b[39mif\u001b[39;00m x \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mORGANIZATION\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     43\u001b[0m \u001b[39m# Filter for words tagged as LOCATION and store it in data frame\u001b[39;00m\n\u001b[0;32m     44\u001b[0m text_out\u001b[39m.\u001b[39mloc[idx,\u001b[39m'\u001b[39m\u001b[39mPlace\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([docNER[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i,x \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(docNER) \u001b[39mif\u001b[39;00m x \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLOCATION\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\core\\indexing.py:818\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    817\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[1;32m--> 818\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\core\\indexing.py:1774\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_maybe_update_cacher(clear\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1772\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_is_copy \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1774\u001b[0m     nindexer\u001b[39m.\u001b[39mappend(labels\u001b[39m.\u001b[39;49mget_loc(key))\n\u001b[0;32m   1776\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1777\u001b[0m     nindexer\u001b[39m.\u001b[39mappend(idx)\n",
      "File \u001b[1;32mc:\\Users\\ameym\\anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m casted_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_indexer(key)\n\u001b[0;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m   3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text_ner_out = {}\n",
    "iters =  100\n",
    "for i in range(iters):\n",
    "  text_ner_out[i] = get_NER(text_out.iloc[i*math.ceil(text_out.size/iters):(i+1)*math.ceil(text_out.size/iters),:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ner_out_full = pd.DataFrame(text_ner_out[0])\n",
    "for i in range(1,iters):\n",
    "  pd.concat([text_ner_out_full, text_ner_out[i]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Place</th>\n",
       "      <th>Person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fetterman's new chief of staff co-founded The ...</td>\n",
       "      <td>Trump-Russia</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>Fetterman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@RyanPet70527777 @realpeterf @NEWSMAX Hes got ...</td>\n",
       "      <td>NEWSMAX-Hes</td>\n",
       "      <td></td>\n",
       "      <td>Donald-Trump,Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@RVAwonk Listen to this 2017 interview by Ginn...</td>\n",
       "      <td>Trump-Intel</td>\n",
       "      <td></td>\n",
       "      <td>Ginni-Thomas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@WhiteHouse when President Trump left office o...</td>\n",
       "      <td>WhiteHouse</td>\n",
       "      <td></td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Elon Musk and Donald Trump are the two lone v...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Elon-Musk,Donald-Trump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Organization   Place  \\\n",
       "0  Fetterman's new chief of staff co-founded The ...  Trump-Russia  Moscow   \n",
       "1  @RyanPet70527777 @realpeterf @NEWSMAX Hes got ...   NEWSMAX-Hes           \n",
       "2  @RVAwonk Listen to this 2017 interview by Ginn...   Trump-Intel           \n",
       "3  @WhiteHouse when President Trump left office o...    WhiteHouse           \n",
       "4  \"Elon Musk and Donald Trump are the two lone v...                         \n",
       "\n",
       "                   Person  \n",
       "0               Fetterman  \n",
       "1      Donald-Trump,Trump  \n",
       "2            Ginni-Thomas  \n",
       "3                   Trump  \n",
       "4  Elon-Musk,Donald-Trump  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the outputs of the ner tagger\n",
    "text_ner_out_full.loc[(text_ner_out_full['Place'] != '') | (text_ner_out_full['Organization'] != '')|(text_ner_out_full['Person'] != '')][['text','Organization','Place','Person']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output  the file for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text_ner_out.to_csv('./processed_data/tweets_topics_sentiment_ner.csv', index=False)\n",
    "text_ner_out = pd.read_csv('./processed_data/tweets_topics_sentiment_ner.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Place</th>\n",
       "      <th>Person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fetterman's new chief of staff co-founded The ...</td>\n",
       "      <td>Trump-Russia</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>Fetterman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@RyanPet70527777 @realpeterf @NEWSMAX Hes got ...</td>\n",
       "      <td>NEWSMAX-Hes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Donald-Trump,Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@RVAwonk Listen to this 2017 interview by Ginn...</td>\n",
       "      <td>Trump-Intel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ginni-Thomas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@WhiteHouse when President Trump left office o...</td>\n",
       "      <td>WhiteHouse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Elon Musk and Donald Trump are the two lone v...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elon-Musk,Donald-Trump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Organization   Place  \\\n",
       "0  Fetterman's new chief of staff co-founded The ...  Trump-Russia  Moscow   \n",
       "1  @RyanPet70527777 @realpeterf @NEWSMAX Hes got ...   NEWSMAX-Hes     NaN   \n",
       "2  @RVAwonk Listen to this 2017 interview by Ginn...   Trump-Intel     NaN   \n",
       "3  @WhiteHouse when President Trump left office o...    WhiteHouse     NaN   \n",
       "4  \"Elon Musk and Donald Trump are the two lone v...           NaN     NaN   \n",
       "\n",
       "                   Person  \n",
       "0               Fetterman  \n",
       "1      Donald-Trump,Trump  \n",
       "2            Ginni-Thomas  \n",
       "3                   Trump  \n",
       "4  Elon-Musk,Donald-Trump  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ner_out.loc[(text_ner_out['Place'] != '') | (text_ner_out['Organization'] != '')|(text_ner_out['Person'] != '')][['text','Organization','Place','Person']].head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e8ce98ebcabbc47cc785825bfa5df55a93888dc4eb67b451e38ff2fd5b02819"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
